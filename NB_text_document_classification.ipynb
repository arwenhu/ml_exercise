{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons\n",
    "import seaborn as sns\n",
    "import PIL.Image as Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn import metrics\n",
    "\n",
    "pd.set_option('display.max_columns', None) # 针对数据集中列比较多的情况，把dataframe中的列全部显示出来\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)  # set default size of plots\n",
    "#plt.rcParams['image.aspect'] = 1.3\n",
    "plt.rcParams['image.interpolation'] = 'nearest' # 设置 interpolation style\n",
    "#plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['savefig.dpi'] = 300  #图片像素\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['axes.unicode_minus'] = False          # 解决保存图像是负号'-'显示为方块的问题\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False #display only a left and bottom box border\n",
    "plt.rcParams['legend.edgecolor'] = 'white'\n",
    "plt.rcParams['axes.axisbelow'] = False\n",
    "plt.rcParams['grid.color'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取文本数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "['text_classification-master/', 'text_classification-master/README.md', 'text_classification-master/text classification/', 'text_classification-master/text classification/stop/', 'text_classification-master/text classification/stop/stopword.txt']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "r = requests.get('https://github.com/cystanford/text_classification/archive/master.zip',stream = True)\n",
    "print(r.status_code)\n",
    "f = io.BytesIO(r.content)\n",
    "#with zipfile.ZipFile(f) as myzip:\n",
    "#    myzip.extractall(path='/Users/mymac/Documents/Task/pandas/lahman-csv_2014-02-14')\n",
    "names = zipfile.ZipFile(f).namelist()\n",
    "print(names[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ? 、 。 “ ” 《 》 ！ ， ： ； ？ 人民 # ### 啊 阿 哎 哎呀 哎哟 唉 俺 俺们 按 按照 吧 吧哒 把 罢了 被 本 本着 比 比方 比如 鄙人 彼 彼此 边 别 别的 别说 并 并且 不比 不成 不单 不但 不独 不管 不光 不过 不仅 不拘 不论 不怕 不然 不如 不特 不惟 不问 不只 朝 朝着 趁 趁着 乘 冲 除 除此之外 除非 除了 此 此间 此外 从 从而 打 待 但 但是 当 当着 到 得 的 的话 等 等等 地 第 叮咚 对 对于 多 多少 而 而况 而且\n"
     ]
    }
   ],
   "source": [
    "def read_txt(filename, encoding='utf-8'):\n",
    "    with zipfile.ZipFile(f) as myzip:\n",
    "        with myzip.open(filename, 'r') as myfile:\n",
    "            lines = myfile.readlines()\n",
    "\n",
    "    # print(filename)\n",
    "    stop_words = [line.strip().decode(encoding, 'ignore')\n",
    "                  for line in lines]  #'ignore'\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "stop_words_2 = read_txt(\n",
    "    'text_classification-master/text classification/stop/stopword.txt',\n",
    "    encoding='utf-8-sig')\n",
    "print(' '.join([x for x in stop_words_2][0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def read_folder(reg_str):\n",
    "    folder_names = list(filter(re.compile(reg_str).match, names))\n",
    "    print(folder_names)\n",
    "    data = []\n",
    "    for ifolder_name in folder_names:\n",
    "        data_files = list(filter(re.compile(ifolder_name + \".+$\").match, names))\n",
    "        # print(data_files[0])\n",
    "        idata = [read_txt(idata_file, encoding='gbk') for idata_file in data_files]\n",
    "        data.append(list(chain(*idata)))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text_classification-master/text classification/train/体育/', 'text_classification-master/text classification/train/女性/', 'text_classification-master/text classification/train/文学/', 'text_classification-master/text classification/train/校园/']\n",
      "['text_classification-master/text classification/test/体育/', 'text_classification-master/text classification/test/女性/', 'text_classification-master/text classification/test/文学/', 'text_classification-master/text classification/test/校园/']\n",
      "['球场禁用招数“少先队员之惩戒”冠', '可以直接到编辑部买，地址，北京体育馆路8号，中国体育报业总社院内，后楼51700：羽毛球杂志木有支付宝，木有财付通，在网上订购不支持货到付款么？那么北京哪个实体店有卖12月的《羽毛球》杂志，或者说，能去你们编辑社买不？地址？转发(3)评论(4)12月10日09:17来自新浪微博', '直播贴阿内尔卡正式加盟申花，最现场连线申花新闻官带来第一手消息，上海男篮2分惜败惨遭4连败，北京延续不败战绩欧洲足坛激战正酣，米兰被弱旅逼平，拜仁重夺联赛榜首风云变幻，霍华德要求离队，湖人退出三方交易更多内容请听11:00的《990体育新闻》934:', '组图：金妍儿黑丝亮相热心公益 OL套装透成熟 http:url.cn/1bNQDP  (分享自 腾讯体育 )$LOTOzf$', '北京23分落后末节大反扑 惜败佛山终结13连胜  http:url.cn/2nGdrq  (分享自 腾讯体育 )$LOTOzf$']\n",
      "________________________________________________________________________________\n",
      "['意大利的新人Edoardo Alescio上周末成为WPT冠军俱乐部的一员。他在WPT威尼斯站点的比赛获得了冠军和奖金17.5万欧元。$LOTOzf$', '李娜真棒 坚持到了最后 国歌在法网比赛中第一次响起 好激动哈$LOTOzf$', '北京时间3月4日，凯尔特人队消息，前锋“大宝贝”格伦-戴维斯将因为左膝髌骨肌腱炎缺席一周。“大宝贝”本赛季场均11.6分5.3篮板，他是在115-103击败太阳队的比赛中受伤的，但伤势并不严重。$LOTOzf$', '是山东队的吴轲，本赛季进步非常大，赛季开始前还入选了范斌执教的国奥队前往甘肃兰州和NBL明星队打了一场全明星赛。$LOTOzf$此微博已被原作者删除。', '有点危险，今天看斯洛文尼亚的记者会，有可能打类似朝鲜阵型（或龟缩战术），因为斯洛文尼亚战平就出线，且是历史上第一次世界杯小组出线，肯定会死守。会上的言语还间接挑衅了鲁尼，称其徒有虚名（心理战），鲁小胖脾气暴，是有目共睹，但愿别急于求成，在比赛中上当。打出配合，赢球不难。$LOTOzf$']\n"
     ]
    }
   ],
   "source": [
    "train = read_folder(\".*train/.+/$\")\n",
    "test = read_folder(\".*test/.+/$\")\n",
    "print(train[0][:5])\n",
    "print('_' * 80)\n",
    "print(test[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/8b/mn7p_n410l5dh_8lxtjy_m680000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports', 'sports', 'sports', 'sports', 'sports']\n",
      "________________________________________________________________________________\n",
      "['sports', 'sports', 'sports', 'sports', 'sports']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.870 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['球场 禁用 招数   队员 少先队 少先队员 之 惩戒   冠', '可以 直接 到 编辑 编辑部 买   地址   北京 体育 体育馆 路   号   中国 体育 报业 总社 院内   后楼         羽毛 羽毛球 杂志 木有 支付 支付宝   木有 财付 通   在 网上 订购 不 支持 付款 货到付款 么   那么 北京 哪个 实体 实体店 有 卖    月 的   羽毛 羽毛球   杂志   或者 或者说   能 去 你们 编辑 社买 不   地址   转发       评论          月    日         来自 新浪 微博', '直播 贴 阿内 内尔 阿内尔 阿内尔卡 正式 加盟 申花   最 现场 连线 申花 新闻 新闻官 带来 第一 一手 第一手 消息   上海 男篮   分 惜败 惨遭   连败   北京 延续 不败 战绩 欧洲 足坛 激战 正酣   米兰 被 弱旅 逼平   拜仁 重夺 联赛 榜首 风云 变幻 风云变幻   霍华德 要求 离队   湖人 退出 三方 交易 更 多 内容 请 听         的       体育 育新 新闻 体育新闻', '组图   金妍儿 黑丝 亮相 热心 公益      套装 透 成熟                                  分享 自   腾讯 体育', '北京    分 落后 末节 大 反扑   惜败 佛山 终结    连胜                                    分享 自   腾讯 体育']\n",
      "________________________________________________________________________________\n",
      "['大利 意大利 的 新人                   上周 周末 上周末 成为     冠军 俱乐部 的 一员   他 在     威尼 尼斯 威尼斯 站点 的 比赛 获得 了 冠军 和 奖金      万 欧元', '李娜 真棒   坚持 到 了 最后   国歌 在 法网 比赛 中 第一 一次 第一次 响起   好 激动 哈', '北京 时间   月   日   凯尔 尔特 凯尔特 凯尔特人 队 消息   前锋   大 宝贝   格伦   维斯 戴维斯 将 因为 左膝 髌骨 肌腱 炎 缺席 一周     大 宝贝   本赛 赛季 本赛季 场均      分     篮板   他 是 在           击败 太阳 太阳队 的 比赛 中 受伤 的   但 伤势 并 不 严重', '是 山东 山东队 的 吴轲   本赛 赛季 本赛季 进步 非常 大   赛季 开始 前 还 入选 了 范斌 执教 的 国奥 国奥队 前往 甘肃 兰州 甘肃兰州 和     明星 明星队 打 了 一场 明星 全明星 全明星赛              此微博 已 被 原作 作者 原作者 删除', '有点 危险   今天 看 尼亚 斯洛文尼亚 的 记者 记者会   有 可能 打 类似 朝鲜 阵型   或 龟缩 战术     因为 尼亚 斯洛文尼亚 战平 就 出线   且 是 历史 上 第一 一次 第一次 世界 世界杯 小组 出线   肯定 会 死守   会上 的 言语 还 间接 挑衅 了 鲁尼   称其 虚名 徒有虚名   心理 心理战     鲁 小胖 脾气 暴   是 共睹 有目共睹   但愿 别 急于 求成 急于求成   在 比赛 中 上当   打出 配合   赢球 不难']\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 分类label\n",
    "categories = ['sports', 'female', 'literature', 'campus']\n",
    "\n",
    "def get_labels(data):\n",
    "    data_labels = [[icategory] * len(idata)\n",
    "                   for idata, icategory in zip(data, categories)]\n",
    "    return list(chain(*data_labels))\n",
    "\n",
    "train_labels = get_labels(train)\n",
    "test_labels = get_labels(test)\n",
    "print(train_labels[:5])\n",
    "print('_' * 80)\n",
    "print(test_labels[:5])\n",
    "\n",
    "# 分类features\n",
    "def get_features(data):\n",
    "    data_chain = list(chain(*data))\n",
    "    # 分词 & 去掉非中文字符\n",
    "    data_chain = [\n",
    "        re.compile(u'[^\\u4E00-\\u9FFF]').sub(\n",
    "            r' ', ' '.join(jieba.cut_for_search(x))).strip()\n",
    "        for x in data_chain\n",
    "    ]\n",
    "    print(data_chain[:5])\n",
    "    print('_' * 80)\n",
    "    return data_chain\n",
    "\n",
    "train_chain = get_features(train)\n",
    "test_chain = get_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 去掉新浪|微博|转发|评论|分享|腾讯|标签|原文\n",
    "train_chain = [re.compile(u'新浪|微博|转发|评论|分享|腾讯').sub(r'', x).strip() for x in train_chain]\n",
    "test_chain = [re.compile(u'新浪|微博|转发|评论|分享|腾讯').sub(r'', x).strip() for x in test_chain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mymac/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一一', '一丁', '一万', '一万元', '一上', '一上午', '一下', '一下下', '一下子', '一下脸', '一丝', '一个个', '一个伍', '一个名', '一个多', '一个多月', '一个月', '一个男孩', '一中', '一串', '一为', '一举', '一举一动', '一举三得', '一举两得', '一书', '一事', '一事无成', '一二', '一二个']\n",
      "(3313, 23507)\n",
      "['一一', '一丁', '一万', '一万元', '一上', '一上午', '一下', '一下下', '一下子', '一下脸', '一丝', '一个个', '一个伍', '一个名', '一个多', '一个多月', '一个月', '一个男孩', '一中', '一串', '一为', '一举', '一举一动', '一举三得', '一举两得', '一书', '一事', '一事无成', '一二', '一二个']\n",
      "(200, 23507)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mymac/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec_train = TfidfVectorizer(\n",
    "    stop_words=stop_words_2, max_df=0.5, analyzer='word') #, ngram_range=(1,2)\n",
    "tfidf_matrix_train = tfidf_vec_train.fit_transform(train_chain)\n",
    "print(tfidf_vec_train.get_feature_names()[:30])\n",
    "train_features = tfidf_matrix_train.toarray()\n",
    "print(train_features.shape)\n",
    "#print(train_features[:5])\n",
    "\n",
    "train_vocabulary = tfidf_vec_train.vocabulary_\n",
    "tfidf_vec_test = TfidfVectorizer(\n",
    "    stop_words=stop_words_2, max_df=0.5, vocabulary=train_vocabulary)\n",
    "tfidf_matrix_test = tfidf_vec_test.fit_transform(test_chain)\n",
    "print(tfidf_vec_train.get_feature_names()[:30])\n",
    "test_features = tfidf_matrix_test.toarray()\n",
    "print(test_features.shape)\n",
    "#print(test_features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['campus', 'female', 'literature', 'sports']\n",
      "________________________________________________________________________________\n",
      "['举行', '标签', '参加', '业生', '教育', '报名', '大学', '学生', '孩子', '老师', '毕业', '毕业生', '招聘', '同学', '原文', '青年', '保护', '绿色', '学院', '小学', '校园', '大学生', '考试', '环保', '就业', '校车', '研究生', '本科', '招生', '考生']\n",
      "________________________________________________________________________________\n",
      "['分钟', '喜欢', '收听', '每天', '原文', '妈妈', '化妆', '减肥', '肌肤', '女人', '美丽', '美容', '注射', '除皱', '蛋白', '皮肤', '整形', '女性', '星座', '化妆品', '面膜', '瘦身', '淘宝', '保湿', '美白', '蜂蜜', '柠檬', '护肤', '指甲', '美甲']\n",
      "________________________________________________________________________________\n",
      "['编辑', '杂志', '标签', '敬请', '周刊', '不错', '原文', '布斯', '阅读', '故事', '作家', '封面', '读者', '上市', '月刊', '图书', '读书', '火花', '小说', '出版社', '新书', '好书', '意林', '一本', '文学', '本书', '订阅', '乔布', '乔布斯', '想读']\n",
      "________________________________________________________________________________\n",
      "['体育', '羽毛', '羽毛球', '直播', '阿内', '内尔', '阿内尔', '阿内尔卡', '联赛', '风云', '科比', '皇马', '冠军', '足球', '决赛', '巴萨', '球员', '标签', '今晚', '比赛', '赛季', '球队', '球迷', '游泳', '英超', '体坛', '篮球', '运动', '俱乐部', '教练']\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tfidfs = pd.concat([\n",
    "    pd.DataFrame(train_features),\n",
    "    pd.DataFrame(train_labels, columns=['label'])\n",
    "],\n",
    "                   axis=1)\n",
    "tfidfs_sum = tfidfs.groupby('label').sum()\n",
    "voc = pd.Series(tfidf_vec_train.vocabulary_)\n",
    "print(tfidfs_sum.index.tolist())\n",
    "print('_' * 80)\n",
    "for i in np.arange(4):\n",
    "    print(voc[voc.isin(\n",
    "        pd.Series(tfidfs_sum.iloc[i, ].sort_values(\n",
    "            ascending=False)[:30].index))].index.tolist())\n",
    "    print('_' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13819</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>campus</th>\n",
       "      <td>3.884964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>literature</th>\n",
       "      <td>0.225690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>0.576862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               13819\n",
       "label               \n",
       "campus      3.884964\n",
       "female      0.000000\n",
       "literature  0.225690\n",
       "sports      0.576862"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfs_sum[voc[[x == u'报名' for x in voc.index.tolist()]]]\n",
    "tfidfs_sum[voc[[x == u'招生' for x in voc.index.tolist()]]]\n",
    "tfidfs_sum[voc[[x == u'论文' for x in voc.index.tolist()]]]\n",
    "tfidfs_sum[voc[[x == u'毕业' for x in voc.index.tolist()]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the MultinomialNB is 0.92\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      campus       0.93      0.81      0.87        16\n",
      "      female       0.97      0.87      0.92        38\n",
      "  literature       0.71      0.97      0.82        31\n",
      "      sports       0.98      0.94      0.96       115\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       200\n",
      "   macro avg       0.90      0.90      0.89       200\n",
      "weighted avg       0.93      0.92      0.92       200\n",
      "\n",
      "[[ 13   0   3   0]\n",
      " [  0  33   4   1]\n",
      " [  0   0  30   1]\n",
      " [  1   1   5 108]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "model = MultinomialNB(alpha=0.01).fit(train_features, train_labels)\n",
    "\n",
    "prediction = model.predict(test_features)\n",
    "print('The accuracy of the MultinomialNB is',\n",
    "      metrics.accuracy_score(prediction, test_labels))\n",
    "print(metrics.classification_report(test_labels, prediction))\n",
    "print(metrics.confusion_matrix(test_labels, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
